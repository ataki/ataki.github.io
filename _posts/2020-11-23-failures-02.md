---
layout: post
title: failure-02
---

Second in a series of posts about failures.

This is about the failure of an architecture, and various attempts to solve it.

# Relevant Reading  
* [Data Monolith to Data Mesh](https://martinfowler.com/articles/data-monolith-to-mesh.html)
* [Narrator.ai, an 11-column ](https://martinfowler.com/articles/data-monolith-to-mesh.html)
* [Emerging Data Architectures, a16z](https://a16z.com/2020/10/15/the-emerging-architectures-for-modern-data-infrastructure/)

# Data Architectures

How do you build data architectures that don't fall down? As far as I know,
that's a problem in the data sector many companies have struggled to answer.
It's a question I've spent the past 6 years thinking about. Why can't we get it
right?

To understand why, we have to go to the root cause. There are two categories,
scale and complexity. Scale is when your data volumes multiply by a factor
of 100 or so. Many teams have spent a decade tackling this problem. Complexity
is when the semantics of your data change. No team as far as I know has
deliberately tried to tackle this problem in an original way.

The closest is Zhamak Dehghani's idea of a "Data Mesh". At its essence,
it's about applying domain-driven design to data architectures. The innovation 
comes from phrasing the problem as an issue with org structure and thinking
rather than a pure tech problem. It's the same with our move to microservices
or service-oriented architecture: it's about the
[domain](https://eng.uber.com/microservice-architecture/), not the technology.

We see a similar pattern over the past decade in services as we do with data.
Back in 2008, when the idea of microservices was (re-)introduced, people focused on
building tech first - thrift, rest frameworks, ci/cd as a service. The problem
of tackling complexity and finding the right boundaries is now the focus. It's
tough when your eng org is in a competitive market and needs to iterate
quickly. A good analogy I've heard is "replacing the engine of a moving car".

Back in data land, the tech is now roughly here. You have companies that offer
stream processing as a service, storage and warehousing, and even analytics
tracking. You can also build your own on the cloud (on-prem is stil fairly
challenging). But tackling complexity is still an issue.

So we're back to the complexity. What does the data mesh promise to solve? One,
it puts domain teams<sup>1</sup> in the driver's seat. **Domain teams now produce and
maintain data as a product**. This is not a new concept, but the 
definition of a "data product" wasn't well-known up until now. Neither was the
understanding of how to maintain one. How do you measure quality except through
ad-hoc scripts? What operational burden can you take away from domain teams?

The verdict's out on whether data mesh is a good abstraction. It certainly
lines up with intuition. We haven't been able to solve the domain
modeling problem beccause the responsibilities are incorrectly aligned. The
natural next question to ask is: who should be responsible? We're not sure.
Domain teams are a good guess, but what if your org isn't neatly divided
up into domain boundaries? Services like Narrator.ai provide an interesting
answer: give the responsibility to folks outside your org. Until, at least,
your eng org can start restructuring.

The other question about a data mesh is: what organizational complexity does
this incur? We don't know the answer to that either. I suspect we'll have
to train domain teams on best practices for modeling, thereby growing the
responsibilities of a software engineering team. For larger organizations, this
might not be possible due to the scale. They'll probably need specialized
consulting services to bridge the gap.

For younger organizations that don't yet have mass market scale, it's a perfect
opportunity to try this out. If we get it right, we'll have answered some
important questions about the nature of data modeling.

---

<sup>1</sup> domain teams are eng teams that support a business domain with
clear boundaries to other teams. they maintain services, pipelines, apps,
and most importantly, their domain models.
